{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOV8kGv8dyNU2NUoljtbgLc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BozOuK6hYP1Q"},"outputs":[],"source":["# Installing kaggle package to work with Kaggle datasets\n","!pip install kaggle"]},{"cell_type":"code","source":["# Creating a directory to store the kaggle.json API key file\n","!mkdir -p ~/.kaggle\n","# Copying the kaggle.json file to the .kaggle folder for authentication\n","!cp kaggle.json ~/.kaggle/\n","# Setting appropriate file permissions for kaggle.json to ensure it's secure\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"oHdPUTpbYSgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Downloading the sentiment140 dataset from Kaggle\n","!kaggle datasets download -d kazanova/sentiment140"],"metadata":{"id":"uGB_mm6uYSc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting the downloaded dataset from the zip file\n","from zipfile import ZipFile\n","dataset = '/content/sentiment140.zip'\n","with ZipFile(dataset, 'r') as zip:\n","  zip.extractall()\n","  print('The dataset is extracted')"],"metadata":{"id":"-h0yriMvYSac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing necessary libraries for data handling and processing\n","import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n"],"metadata":{"id":"4vFJNvMyYSXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing NLP-specific libraries for text preprocessing\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer"],"metadata":{"id":"kMmQJEPDYSVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing libraries for feature extraction, model training, and performance evaluation\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"KKOzoVZLYSSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Downloading stopwords for preprocessing (removing common words like \"the\", \"is\", etc.)\n","nltk.download('stopwords')\n"],"metadata":{"id":"Pk3CjpQcYSQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the list of English stopwords\n","print(stopwords.words('english'))"],"metadata":{"id":"RyPeTqeMYSNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the dataset into a pandas dataframe\n","twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')\n"],"metadata":{"id":"_gEipxvyYSK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the number of rows and columns in the dataset\n","twitter_data.shape\n"],"metadata":{"id":"-peW0erGYSIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the first few rows of the dataset to understand its structure\n","twitter_data.head()"],"metadata":{"id":"81jXhb2cYSFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renaming the columns to make them more intuitive\n","column_names = ['target', 'date', 'id', 'flag', 'user', 'text']\n","twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', names=column_names, encoding='ISO-8859-1')\n"],"metadata":{"id":"Bngp7jCtYSDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the updated shape of the dataset\n","twitter_data.shape"],"metadata":{"id":"x6YnD1qOYSAg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the first few rows after renaming columns\n","twitter_data.head()"],"metadata":{"id":"WQ9lC1pJYR9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking for any missing values in the dataset\n","twitter_data.isnull().sum()"],"metadata":{"id":"BTFoMpuvYR66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the distribution of the target column (0 = negative, 4 = positive sentiment)\n","twitter_data['target'].value_counts()\n"],"metadata":{"id":"a-76RZsOYR4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting the target value of 4 (positive sentiment) to 1 for binary classification\n","twitter_data.replace({'target': {4: 1}}, inplace=True)"],"metadata":{"id":"tVaO6L2bYR1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verifying the updated target column distribution (0 = negative, 1 = positive)\n","twitter_data['target'].value_counts()"],"metadata":{"id":"bQeqS8AVYRy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing the PorterStemmer for stemming (reducing words to their root form)\n","port_stem = PorterStemmer()\n"],"metadata":{"id":"ZW9Gn9_bYRwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining a function to clean and stem the text data\n","def stemming(content):\n","    # Removing all characters except alphabets\n","    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n","    # Converting the text to lowercase\n","    stemmed_content = stemmed_content.lower()\n","    # Splitting the text into individual words\n","    stemmed_content = stemmed_content.split()\n","    # Stemming each word and removing stopwords\n","    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n","    # Joining the stemmed words back into a sentence\n","    stemmed_content = ' '.join(stemmed_content)\n","    return stemmed_content\n"],"metadata":{"id":"yaDrCkT5YRt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applying the stemming function to the 'text' column and creating a new column for processed content\n","twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)\n","# And it take around half an hour to load so dont thinks that something wrong in code"],"metadata":{"id":"zHa7JAy-YRrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the first few rows of the stemmed content\n","twitter_data.head()"],"metadata":{"id":"zVwOJU7ZZlYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the processed 'stemmed_content' column\n","print(twitter_data['stemmed_content'])"],"metadata":{"id":"i9busBXoYRon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the 'target' column (sentiment labels)\n","print(twitter_data['target'])"],"metadata":{"id":"usrmfbPOYRl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separating the features (X) and labels (Y) from the dataset\n","X = twitter_data['stemmed_content'].values\n","Y = twitter_data['target'].values"],"metadata":{"id":"b2adrvCyZe1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the feature data (X)\n","print(X)\n"],"metadata":{"id":"T2KNT6oPZexu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the label data (Y)\n","print(Y)\n"],"metadata":{"id":"w3Ytn4fwZ1uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the data into training and testing sets (80% training, 20% testing)\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)\n"],"metadata":{"id":"4w39qh6XZ1qy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the shapes of the training and testing sets\n","print(X.shape, X_train.shape, X_test.shape)\n"],"metadata":{"id":"xyKxbYHLZ1oZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying a sample of the training data\n","print(X_train)\n"],"metadata":{"id":"SHDxosL-Z-sy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying a sample of the testing data\n","print(X_test)"],"metadata":{"id":"z3Tl6jL0Z-pW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing the TfidfVectorizer to convert text data into numerical format (TF-IDF vectors)\n","vectorizer = TfidfVectorizer()"],"metadata":{"id":"26qw59UGZ-kY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fitting the vectorizer on the training data and transforming it into TF-IDF features\n","X_train = vectorizer.fit_transform(X_train)"],"metadata":{"id":"zHwBORxwZ-g-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transforming the test data into TF-IDF features using the same vectorizer\n","X_test = vectorizer.transform(X_test)"],"metadata":{"id":"gkq2Q_U7Z-ew"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the transformed training data (sparse matrix format)\n","print(X_train)"],"metadata":{"id":"r8hKVJnEZ-b4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying the transformed test data (sparse matrix format)\n","print(X_test)"],"metadata":{"id":"61ItKI1-ZeuY"},"execution_count":null,"outputs":[]}]}